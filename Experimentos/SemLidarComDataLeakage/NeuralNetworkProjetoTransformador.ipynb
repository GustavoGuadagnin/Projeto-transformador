{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustavocoradin/Projeto-transformador/blob/main/NeuralNetworkProjetoTransformador.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_imports"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer\n",
        "import time\n",
        "import psutil\n",
        "from memory_profiler import memory_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_load_kaggle"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"usdot/flight-delays\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_load_data"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(path + \"/flights.csv\")\n",
        "print(f\"Dataset carregado com shape: {df.shape}\")\n",
        "print(f\"Colunas: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_data_cleaning"
      },
      "outputs": [],
      "source": [
        "print(\"Valores nulos por coluna (%):\")\n",
        "print(df.isna().sum() * 100 / len(df))\n",
        "\n",
        "delay_reason_cols = ['AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY']\n",
        "df[delay_reason_cols] = df[delay_reason_cols].fillna(0)\n",
        "\n",
        "df.drop(['CANCELLATION_REASON', 'FLIGHT_NUMBER', 'CANCELLED'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "df.drop('TAIL_NUMBER', axis=1, inplace=True)\n",
        "\n",
        "df.drop(['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "df = df.dropna(subset=['DEPARTURE_TIME','DEPARTURE_DELAY','TAXI_OUT','WHEELS_OFF','SCHEDULED_TIME','ELAPSED_TIME','AIR_TIME','WHEELS_ON','TAXI_IN','ARRIVAL_TIME','ARRIVAL_DELAY'])\n",
        "\n",
        "print(f\"\\nShape ap√≥s limpeza: {df.shape}\")\n",
        "print(\"\\nValores nulos restantes (%):\")\n",
        "print(df.isna().sum() * 100 / len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_create_target"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['DELAYED'] = df['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 15 else 0)\n",
        "\n",
        "delay = df[df['DELAYED'] == 1]\n",
        "on_time = df[df['DELAYED'] == 0]\n",
        "\n",
        "delay_count = delay.shape[0]\n",
        "on_time_count = on_time.shape[0]\n",
        "total = delay_count + on_time_count\n",
        "delay_percentage = (delay_count / total) * 100\n",
        "on_time_percentage = (on_time_count / total) * 100\n",
        "\n",
        "print(f\"N√£o atrasados: {on_time_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Atrasados: {delay_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Percentual de voos n√£o atrasados: {on_time_percentage:.2f}%\")\n",
        "print(f\"Percentual de voos atrasados: {delay_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_undersampling"
      },
      "outputs": [],
      "source": [
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X = df.drop(columns=['DELAYED'])\n",
        "y = df['DELAYED']\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "df_undersampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "on_time_count = (df_undersampled['DELAYED'] == 0).sum()\n",
        "delay_count = (df_undersampled['DELAYED'] == 1).sum()\n",
        "total = on_time_count + delay_count\n",
        "on_time_percentage = (on_time_count / total) * 100\n",
        "delay_percentage = (delay_count / total) * 100\n",
        "\n",
        "print(f\"\\nAp√≥s undersampling:\")\n",
        "print(f\"N√£o atrasados: {on_time_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Atrasados: {delay_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Percentual de voos n√£o atrasados: {on_time_percentage:.2f}%\")\n",
        "print(f\"Percentual de voos atrasados: {delay_percentage:.2f}%\")\n",
        "print(f\"Shape do dataset balanceado: {df_undersampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_sample_data"
      },
      "outputs": [],
      "source": [
        "df_delayed = df_undersampled[df_undersampled['DELAYED'] == 1]\n",
        "df_on_time = df_undersampled[df_undersampled['DELAYED'] == 0]\n",
        "\n",
        "sample_size_per_class = 1000000\n",
        "\n",
        "df_delayed_sample = df_delayed.sample(n=sample_size_per_class, random_state=42)\n",
        "df_on_time_sample = df_on_time.sample(n=sample_size_per_class, random_state=42)\n",
        "\n",
        "df_sample = pd.concat([df_delayed_sample, df_on_time_sample])\n",
        "\n",
        "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Shape do DataFrame original:\", df_undersampled.shape)\n",
        "print(\"Shape do DataFrame amostrado:\", df_sample.shape)\n",
        "print(\"\\nDistribui√ß√£o da classe 'DELAYED' no DataFrame amostrado:\")\n",
        "print(df_sample['DELAYED'].value_counts())\n",
        "print(f\"\\n‚úÖ Neural Networks podem processar {sample_size_per_class*2:,} amostras com excelente escalabilidade\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_split_data"
      },
      "outputs": [],
      "source": [
        "X_sample = df_sample.drop(columns=['DELAYED'])\n",
        "y_sample = df_sample['DELAYED']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample, y_sample,\n",
        "    test_size=0.3,\n",
        "    stratify=y_sample,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Tamanho do conjunto de treino: {X_train.shape[0]}\")\n",
        "print(f\"Tamanho do conjunto de teste: {X_test.shape[0]}\")\n",
        "print(f\"Distribui√ß√£o no treino: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Distribui√ß√£o no teste: {y_test.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_normalize"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"‚úÖ Normaliza√ß√£o conclu√≠da com StandardScaler\")\n",
        "print(f\"M√©dia das features de treino ap√≥s normaliza√ß√£o: {X_train_scaled.mean().mean():.6f}\")\n",
        "print(f\"Desvio padr√£o das features de treino ap√≥s normaliza√ß√£o: {X_train_scaled.std().mean():.6f}\")\n",
        "print(f\"\\nüìä Estat√≠sticas p√≥s-normaliza√ß√£o (treino):\")\n",
        "print(f\"Min: {X_train_scaled.min().min():.3f}\")\n",
        "print(f\"Max: {X_train_scaled.max().max():.3f}\")\n",
        "print(f\"M√©dia: {X_train_scaled.mean().mean():.6f}\")\n",
        "print(f\"Std: {X_train_scaled.std().mean():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_train"
      },
      "outputs": [],
      "source": [
        "\n",
        "nn_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),   \n",
        "    activation='relu',              \n",
        "    solver='adam',                 \n",
        "    learning_rate_init=0.001,       \n",
        "    max_iter=500,                \n",
        "    random_state=42,\n",
        "    early_stopping=True,            \n",
        "    validation_fraction=0.1,       \n",
        "    n_iter_no_change=10              \n",
        ")\n",
        "\n",
        "start_time_train = time.time()\n",
        "nn_model.fit(X_train_scaled, y_train)\n",
        "end_time_train = time.time()\n",
        "\n",
        "training_time = end_time_train - start_time_train\n",
        "print(f\"\\nTempo de Treinamento: {training_time:.4f} segundos\")\n",
        "print(f\"Modelo Neural Network treinado com arquitetura: {nn_model.hidden_layer_sizes}\")\n",
        "print(f\"N√∫mero de itera√ß√µes realizadas: {nn_model.n_iter_}\")\n",
        "print(f\"Fun√ß√£o de ativa√ß√£o: {nn_model.activation}\")\n",
        "print(f\"Otimizador: {nn_model.solver}\")\n",
        "print(f\"Taxa de aprendizado: {nn_model.learning_rate_init}\")\n",
        "print(f\"Early stopping ativado: {nn_model.early_stopping}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_predict"
      },
      "outputs": [],
      "source": [
        "start_time_pred = time.time()\n",
        "y_pred = nn_model.predict(X_test_scaled)\n",
        "end_time_pred = time.time()\n",
        "\n",
        "prediction_time = end_time_pred - start_time_pred\n",
        "print(f\"Tempo de Predi√ß√£o: {prediction_time:.4f} segundos\")\n",
        "print(f\"Predi√ß√µes realizadas para {len(X_test_scaled)} amostras\")\n",
        "\n",
        "y_pred_proba = nn_model.predict_proba(X_test_scaled)\n",
        "print(f\"\\nüìä Probabilidades calculadas para an√°lise de confian√ßa\")\n",
        "print(f\"Probabilidade m√©dia para classe 0: {y_pred_proba[:, 0].mean():.4f}\")\n",
        "print(f\"Probabilidade m√©dia para classe 1: {y_pred_proba[:, 1].mean():.4f}\")\n",
        "\n",
        "\n",
        "print(f\"\\nüîÑ Informa√ß√µes de Converg√™ncia:\")\n",
        "print(f\"Loss final: {nn_model.loss_:.6f}\")\n",
        "print(f\"Convergiu: {'Sim' if nn_model.n_iter_ < nn_model.max_iter else 'N√£o (max_iter atingido)'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_metrics"
      },
      "outputs": [],
      "source": [
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "tpr = recall \n",
        "tnr = tn / (tn + fp)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"=== M√âTRICAS NO CONJUNTO DE TESTE ===\")\n",
        "print(f\"Acur√°cia: {accuracy:.4f}\")\n",
        "print(f\"Precis√£o: {precision:.4f}\")\n",
        "print(f\"Recall (TPR): {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"True Negative Rate (TNR): {tnr:.4f}\")\n",
        "print(\"\\nMatriz de Confus√£o:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_train_metrics"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train_pred = nn_model.predict(X_train_scaled)\n",
        "\n",
        "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
        "precision_train = precision_score(y_train, y_train_pred)\n",
        "recall_train = recall_score(y_train, y_train_pred)\n",
        "f1_train = f1_score(y_train, y_train_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
        "tpr_train = recall_train\n",
        "tnr_train = tn / (tn + fp)\n",
        "\n",
        "print(\"=== M√âTRICAS NO CONJUNTO DE TREINO ===\")\n",
        "print(f\"Acur√°cia: {accuracy_train:.4f}\")\n",
        "print(f\"Precis√£o: {precision_train:.4f}\")\n",
        "print(f\"Recall (TPR): {recall_train:.4f}\")\n",
        "print(f\"F1-score: {f1_train:.4f}\")\n",
        "print(f\"True Negative Rate (TNR): {tnr_train:.4f}\")\n",
        "\n",
        "print(\"\\n=== COMPARA√á√ÉO TREINO vs TESTE ===\")\n",
        "print(f\"Diferen√ßa de Acur√°cia: {accuracy_train - accuracy:.4f}\")\n",
        "print(f\"Diferen√ßa de F1-score: {f1_train - f1:.4f}\")\n",
        "\n",
        "# Detectando overfitting\n",
        "acc_diff = accuracy_train - accuracy\n",
        "if acc_diff > 0.05:\n",
        "    print(f\"\\n‚ö†Ô∏è  POSS√çVEL OVERFITTING DETECTADO!\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-teste: {acc_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Modelo parece estar generalizando bem.\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-teste: {acc_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_cv"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALIDA√á√ÉO CRUZADA 5-FOLDS - NEURAL NETWORK\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score)\n",
        "}\n",
        "\n",
        "nn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('nn', MLPClassifier(\n",
        "        hidden_layer_sizes=(64, 32),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        n_iter_no_change=10\n",
        "    ))\n",
        "])\n",
        "\n",
        "cv_results = cross_validate(nn_pipeline, X_sample, y_sample, cv=cv, scoring=scoring, return_train_score=True)\n",
        "\n",
        "print(f\"Acur√°cia - Treino: {cv_results['train_accuracy'].mean():.4f} ¬± {cv_results['train_accuracy'].std():.4f}\")\n",
        "print(f\"Acur√°cia - Valida√ß√£o: {cv_results['test_accuracy'].mean():.4f} ¬± {cv_results['test_accuracy'].std():.4f}\")\n",
        "print(f\"Precis√£o - Treino: {cv_results['train_precision'].mean():.4f} ¬± {cv_results['train_precision'].std():.4f}\")\n",
        "print(f\"Precis√£o - Valida√ß√£o: {cv_results['test_precision'].mean():.4f} ¬± {cv_results['test_precision'].std():.4f}\")\n",
        "print(f\"Recall - Treino: {cv_results['train_recall'].mean():.4f} ¬± {cv_results['train_recall'].std():.4f}\")\n",
        "print(f\"Recall - Valida√ß√£o: {cv_results['test_recall'].mean():.4f} ¬± {cv_results['test_recall'].std():.4f}\")\n",
        "print(f\"F1-score - Treino: {cv_results['train_f1'].mean():.4f} ¬± {cv_results['train_f1'].std():.4f}\")\n",
        "print(f\"F1-score - Valida√ß√£o: {cv_results['test_f1'].mean():.4f} ¬± {cv_results['test_f1'].std():.4f}\")\n",
        "\n",
        "cv_acc_diff = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
        "if cv_acc_diff > 0.05:\n",
        "    print(f\"\\n‚ö†Ô∏è  POSS√çVEL OVERFITTING DETECTADO NA VALIDA√á√ÉO CRUZADA!\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-valida√ß√£o: {cv_acc_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Modelo generalizando bem na valida√ß√£o cruzada.\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-valida√ß√£o: {cv_acc_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn_performance"
      },
      "outputs": [],
      "source": [
        "process = psutil.Process()\n",
        "nn_perf = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=10\n",
        ")\n",
        "\n",
        "\n",
        "def train_nn_model():\n",
        "    nn_perf.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "start_time_train = time.time()\n",
        "mem_usage_train = memory_usage(train_nn_model)\n",
        "end_time_train = time.time()\n",
        "training_time = end_time_train - start_time_train\n",
        "train_ips = len(X_train_scaled) / training_time\n",
        "\n",
        "def predict_nn_model():\n",
        "    global y_pred_perf\n",
        "    y_pred_perf = nn_perf.predict(X_test_scaled)\n",
        "\n",
        "cpu_percent_before = process.cpu_percent(interval=None)\n",
        "start_time_pred = time.time()\n",
        "mem_usage_pred = memory_usage(predict_nn_model)\n",
        "end_time_pred = time.time()\n",
        "cpu_percent_after = process.cpu_percent(interval=None)\n",
        "\n",
        "prediction_time = end_time_pred - start_time_pred\n",
        "pred_ips = len(X_test_scaled) / prediction_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"AN√ÅLISE DE DESEMPENHO COMPUTACIONAL - NEURAL NETWORK\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üïí Tempo de Treinamento: {training_time:.4f} s\")\n",
        "print(f\"üïí Tempo de Predi√ß√£o: {prediction_time:.4f} s\")\n",
        "print(f\"üìà Mem√≥ria (Treinamento): {max(mem_usage_train):.2f} MB\")\n",
        "print(f\"üìà Mem√≥ria (Predi√ß√£o): {max(mem_usage_pred):.2f} MB\")\n",
        "print(f\"‚öôÔ∏è CPU usada na predi√ß√£o: {cpu_percent_after:.2f}%\")\n",
        "print(f\"üìä Inst√¢ncias por segundo (treinamento): {train_ips:.2f}\")\n",
        "print(f\"üìä Inst√¢ncias por segundo (predi√ß√£o): {pred_ips:.2f}\")\n",
        "print(f\"üß† Itera√ß√µes realizadas: {nn_perf.n_iter_}\")\n",
        "print(f\"üß† Loss final: {nn_perf.loss_:.6f}\")\n",
        "\n",
        "daily_predictions = 24 * 60 * 60 * pred_ips  # predi√ß√µes por dia\n",
        "print(f\"\\nüìà THROUGHPUT PARA CEN√ÅRIOS REAIS:\")\n",
        "print(f\"Predi√ß√µes por segundo: {pred_ips:.0f}\")\n",
        "print(f\"Predi√ß√µes por minuto: {pred_ips * 60:.0f}\")\n",
        "print(f\"Predi√ß√µes por hora: {pred_ips * 3600:.0f}\")\n",
        "print(f\"Predi√ß√µes por dia: {daily_predictions:.0f}\")\n",
        "\n",
        "print(f\"\\nüß† CARACTER√çSTICAS DA ARQUITETURA:\")\n",
        "print(f\"Camadas ocultas: {nn_perf.hidden_layer_sizes}\")\n",
        "print(f\"Total de par√¢metros: {sum([layer.size for layer in nn_perf.coefs_]) + sum([layer.size for layer in nn_perf.intercepts_])}\")\n",
        "print(f\"Fun√ß√£o de ativa√ß√£o: {nn_perf.activation}\")\n",
        "print(f\"Otimizador: {nn_perf.solver}\")\n",
        "print(f\"Early stopping: {nn_perf.early_stopping}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
