{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_imports"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer\n",
        "import time\n",
        "import psutil\n",
        "from memory_profiler import memory_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_load_kaggle"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"usdot/flight-delays\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_load_data"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(path + \"/flights.csv\")\n",
        "print(f\"Dataset carregado com shape: {df.shape}\")\n",
        "print(f\"Colunas: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_data_cleaning"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Valores nulos por coluna (%):\")\n",
        "print(df.isna().sum() * 100 / len(df))\n",
        "\n",
        "delay_reason_cols = ['AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY']\n",
        "df[delay_reason_cols] = df[delay_reason_cols].fillna(0)\n",
        "\n",
        "\n",
        "df.drop(['CANCELLATION_REASON', 'FLIGHT_NUMBER', 'CANCELLED'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "df.drop('TAIL_NUMBER', axis=1, inplace=True)\n",
        "\n",
        "df.drop(['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "df = df.dropna(subset=['DEPARTURE_TIME','DEPARTURE_DELAY','TAXI_OUT','WHEELS_OFF','SCHEDULED_TIME','ELAPSED_TIME','AIR_TIME','WHEELS_ON','TAXI_IN','ARRIVAL_TIME','ARRIVAL_DELAY'])\n",
        "\n",
        "print(f\"\\nShape ap√≥s limpeza: {df.shape}\")\n",
        "print(\"\\nValores nulos restantes (%):\")\n",
        "print(df.isna().sum() * 100 / len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_create_target"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['DELAYED'] = df['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 15 else 0)\n",
        "\n",
        "delay = df[df['DELAYED'] == 1]\n",
        "on_time = df[df['DELAYED'] == 0]\n",
        "\n",
        "delay_count = delay.shape[0]\n",
        "on_time_count = on_time.shape[0]\n",
        "total = delay_count + on_time_count\n",
        "delay_percentage = (delay_count / total) * 100\n",
        "on_time_percentage = (on_time_count / total) * 100\n",
        "\n",
        "print(f\"N√£o atrasados: {on_time_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Atrasados: {delay_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Percentual de voos n√£o atrasados: {on_time_percentage:.2f}%\")\n",
        "print(f\"Percentual de voos atrasados: {delay_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_undersampling"
      },
      "outputs": [],
      "source": [
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X = df.drop(columns=['DELAYED'])\n",
        "y = df['DELAYED']\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "df_undersampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "\n",
        "on_time_count = (df_undersampled['DELAYED'] == 0).sum()\n",
        "delay_count = (df_undersampled['DELAYED'] == 1).sum()\n",
        "total = on_time_count + delay_count\n",
        "on_time_percentage = (on_time_count / total) * 100\n",
        "delay_percentage = (delay_count / total) * 100\n",
        "\n",
        "print(f\"\\nAp√≥s undersampling:\")\n",
        "print(f\"N√£o atrasados: {on_time_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Atrasados: {delay_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Percentual de voos n√£o atrasados: {on_time_percentage:.2f}%\")\n",
        "print(f\"Percentual de voos atrasados: {delay_percentage:.2f}%\")\n",
        "print(f\"Shape do dataset balanceado: {df_undersampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_sample_data"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_delayed = df_undersampled[df_undersampled['DELAYED'] == 1]\n",
        "df_on_time = df_undersampled[df_undersampled['DELAYED'] == 0]\n",
        "\n",
        "sample_size_per_class = 25000 \n",
        "\n",
        "df_delayed_sample = df_delayed.sample(n=sample_size_per_class, random_state=42)\n",
        "df_on_time_sample = df_on_time.sample(n=sample_size_per_class, random_state=42)\n",
        "\n",
        "\n",
        "df_sample = pd.concat([df_delayed_sample, df_on_time_sample])\n",
        "\n",
        "\n",
        "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Shape do DataFrame original:\", df_undersampled.shape)\n",
        "print(\"Shape do DataFrame amostrado:\", df_sample.shape)\n",
        "print(\"\\nDistribui√ß√£o da classe 'DELAYED' no DataFrame amostrado:\")\n",
        "print(df_sample['DELAYED'].value_counts())\n",
        "print(f\"\\n‚ö†Ô∏è Limita√ß√£o para {sample_size_per_class*2:,} amostras devido √† complexidade computacional O(n¬≤-n¬≥) do SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_split_data"
      },
      "outputs": [],
      "source": [
        "X_sample = df_sample.drop(columns=['DELAYED'])\n",
        "y_sample = df_sample['DELAYED']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample, y_sample,\n",
        "    test_size=0.3,\n",
        "    stratify=y_sample,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Tamanho do conjunto de treino: {X_train.shape[0]}\")\n",
        "print(f\"Tamanho do conjunto de teste: {X_test.shape[0]}\")\n",
        "print(f\"Distribui√ß√£o no treino: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Distribui√ß√£o no teste: {y_test.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_normalize"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"Normaliza√ß√£o conclu√≠da com StandardScaler\")\n",
        "print(f\"Shape treino normalizado: {X_train_scaled.shape}\")\n",
        "print(f\"Shape teste normalizado: {X_test_scaled.shape}\")\n",
        "print(f\"\\nEstat√≠sticas ap√≥s normaliza√ß√£o (treino):\")\n",
        "print(f\"M√©dia: {X_train_scaled.mean().mean():.6f}\")\n",
        "print(f\"Desvio padr√£o: {X_train_scaled.std().mean():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_train"
      },
      "outputs": [],
      "source": [
        "\n",
        "svm = SVC(\n",
        "    kernel='rbf',      \n",
        "    C=1.0,           \n",
        "    gamma='scale',     \n",
        "    random_state=42,\n",
        "    probability=True    \n",
        ")\n",
        "\n",
        "start_time_train = time.time()\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "end_time_train = time.time()\n",
        "\n",
        "training_time = end_time_train - start_time_train\n",
        "print(f\"Tempo de Treinamento: {training_time:.4f} segundos\")\n",
        "print(f\"Modelo SVM treinado com kernel '{svm.kernel}'\")\n",
        "print(f\"Par√¢metro C: {svm.C}\")\n",
        "print(f\"Par√¢metro gamma: {svm.gamma}\")\n",
        "print(f\"N√∫mero de vetores de suporte: {svm.n_support_}\")\n",
        "print(f\"Total de vetores de suporte: {svm.support_vectors_.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_predict"
      },
      "outputs": [],
      "source": [
        "\n",
        "start_time_pred = time.time()\n",
        "y_pred = svm.predict(X_test_scaled)\n",
        "end_time_pred = time.time()\n",
        "\n",
        "prediction_time = end_time_pred - start_time_pred\n",
        "print(f\"Tempo de Predi√ß√£o: {prediction_time:.4f} segundos\")\n",
        "print(f\"Predi√ß√µes realizadas para {len(X_test_scaled)} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_metrics"
      },
      "outputs": [],
      "source": [
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "tpr = recall  \n",
        "tnr = tn / (tn + fp)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"=== M√âTRICAS NO CONJUNTO DE TESTE ===\")\n",
        "print(f\"Acur√°cia: {accuracy:.4f}\")\n",
        "print(f\"Precis√£o: {precision:.4f}\")\n",
        "print(f\"Recall (TPR): {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"True Negative Rate (TNR): {tnr:.4f}\")\n",
        "print(\"\\nMatriz de Confus√£o:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_train_metrics"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train_pred = svm.predict(X_train_scaled)\n",
        "\n",
        "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
        "precision_train = precision_score(y_train, y_train_pred)\n",
        "recall_train = recall_score(y_train, y_train_pred)\n",
        "f1_train = f1_score(y_train, y_train_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
        "tpr_train = recall_train\n",
        "tnr_train = tn / (tn + fp)\n",
        "\n",
        "print(\"=== M√âTRICAS NO CONJUNTO DE TREINO ===\")\n",
        "print(f\"Acur√°cia: {accuracy_train:.4f}\")\n",
        "print(f\"Precis√£o: {precision_train:.4f}\")\n",
        "print(f\"Recall (TPR): {recall_train:.4f}\")\n",
        "print(f\"F1-score: {f1_train:.4f}\")\n",
        "print(f\"True Negative Rate (TNR): {tnr_train:.4f}\")\n",
        "\n",
        "print(\"\\n=== COMPARA√á√ÉO TREINO vs TESTE ===\")\n",
        "print(f\"Diferen√ßa de Acur√°cia: {accuracy_train - accuracy:.4f}\")\n",
        "print(f\"Diferen√ßa de F1-score: {f1_train - f1:.4f}\")\n",
        "\n",
        "# Detectando overfitting\n",
        "acc_diff = accuracy_train - accuracy\n",
        "if acc_diff > 0.05:\n",
        "    print(f\"\\n‚ö†Ô∏è  POSS√çVEL OVERFITTING DETECTADO!\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-teste: {acc_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Modelo parece estar generalizando bem.\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-teste: {acc_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_cv"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALIDA√á√ÉO CRUZADA 5-FOLDS - SVM\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚ö†Ô∏è Processo pode ser demorado devido √† complexidade do SVM...\")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score)\n",
        "}\n",
        "\n",
        "svm_cv = SVC(\n",
        "    kernel='rbf', \n",
        "    C=1.0, \n",
        "    gamma='scale', \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "X_sample_scaled = scaler.fit_transform(X_sample)\n",
        "X_sample_scaled = pd.DataFrame(X_sample_scaled, columns=X_sample.columns)\n",
        "\n",
        "cv_results = cross_validate(svm_cv, X_sample_scaled, y_sample, cv=cv, scoring=scoring, return_train_score=True)\n",
        "\n",
        "print(f\"Acur√°cia - Treino: {cv_results['train_accuracy'].mean():.4f} ¬± {cv_results['train_accuracy'].std():.4f}\")\n",
        "print(f\"Acur√°cia - Valida√ß√£o: {cv_results['test_accuracy'].mean():.4f} ¬± {cv_results['test_accuracy'].std():.4f}\")\n",
        "print(f\"Precis√£o - Treino: {cv_results['train_precision'].mean():.4f} ¬± {cv_results['train_precision'].std():.4f}\")\n",
        "print(f\"Precis√£o - Valida√ß√£o: {cv_results['test_precision'].mean():.4f} ¬± {cv_results['test_precision'].std():.4f}\")\n",
        "print(f\"Recall - Treino: {cv_results['train_recall'].mean():.4f} ¬± {cv_results['train_recall'].std():.4f}\")\n",
        "print(f\"Recall - Valida√ß√£o: {cv_results['test_recall'].mean():.4f} ¬± {cv_results['test_recall'].std():.4f}\")\n",
        "print(f\"F1-score - Treino: {cv_results['train_f1'].mean():.4f} ¬± {cv_results['train_f1'].std():.4f}\")\n",
        "print(f\"F1-score - Valida√ß√£o: {cv_results['test_f1'].mean():.4f} ¬± {cv_results['test_f1'].std():.4f}\")\n",
        "cv_acc_diff = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
        "if cv_acc_diff > 0.05:\n",
        "    print(f\"\\n‚ö†Ô∏è  POSS√çVEL OVERFITTING DETECTADO NA VALIDA√á√ÉO CRUZADA!\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-valida√ß√£o: {cv_acc_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Modelo generalizando bem na valida√ß√£o cruzada.\")\n",
        "    print(f\"Diferen√ßa de acur√°cia treino-valida√ß√£o: {cv_acc_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_performance"
      },
      "outputs": [],
      "source": [
        "process = psutil.Process()\n",
        "\n",
        "svm_perf = SVC(\n",
        "    kernel='rbf', \n",
        "    C=1.0, \n",
        "    gamma='scale', \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "def train_svm_model():\n",
        "    svm_perf.fit(X_train_scaled, y_train)\n",
        "\n",
        "start_time_train = time.time()\n",
        "mem_usage_train = memory_usage(train_svm_model)\n",
        "end_time_train = time.time()\n",
        "training_time = end_time_train - start_time_train\n",
        "train_ips = len(X_train_scaled) / training_time\n",
        "\n",
        "\n",
        "def predict_svm_model():\n",
        "    global y_pred_perf\n",
        "    y_pred_perf = svm_perf.predict(X_test_scaled)\n",
        "\n",
        "cpu_percent_before = process.cpu_percent(interval=None)\n",
        "start_time_pred = time.time()\n",
        "mem_usage_pred = memory_usage(predict_svm_model)\n",
        "end_time_pred = time.time()\n",
        "cpu_percent_after = process.cpu_percent(interval=None)\n",
        "\n",
        "prediction_time = end_time_pred - start_time_pred\n",
        "pred_ips = len(X_test_scaled) / prediction_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"AN√ÅLISE DE DESEMPENHO COMPUTACIONAL - SVM\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üïí Tempo de Treinamento: {training_time:.4f} s\")\n",
        "print(f\"üïí Tempo de Predi√ß√£o: {prediction_time:.4f} s\")\n",
        "print(f\"üìà Mem√≥ria (Treinamento): {max(mem_usage_train):.2f} MB\")\n",
        "print(f\"üìà Mem√≥ria (Predi√ß√£o): {max(mem_usage_pred):.2f} MB\")\n",
        "print(f\"‚öôÔ∏è CPU usada na predi√ß√£o: {cpu_percent_after:.2f}%\")\n",
        "print(f\"üìä Inst√¢ncias por segundo (treinamento): {train_ips:.2f}\")\n",
        "print(f\"üìä Inst√¢ncias por segundo (predi√ß√£o): {pred_ips:.2f}\")\n",
        "print(f\"üî¢ Vetores de suporte: {svm_perf.support_vectors_.shape[0]} ({svm_perf.support_vectors_.shape[0]/len(X_train_scaled)*100:.1f}% dos dados)\")\n",
        "\n",
        "daily_predictions = 24 * 60 * 60 * pred_ips  # predi√ß√µes por dia\n",
        "print(f\"\\nüìà THROUGHPUT PARA CEN√ÅRIOS REAIS:\")\n",
        "print(f\"Predi√ß√µes por segundo: {pred_ips:.0f}\")\n",
        "print(f\"Predi√ß√µes por minuto: {pred_ips * 60:.0f}\")\n",
        "print(f\"Predi√ß√µes por hora: {pred_ips * 3600:.0f}\")\n",
        "print(f\"Predi√ß√µes por dia: {daily_predictions:.0f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è LIMITA√á√ïES COMPUTACIONAIS:\")\n",
        "print(f\"Dataset limitado a {len(X_sample):,} amostras devido √† complexidade O(n¬≤-n¬≥)\")\n",
        "print(f\"Para 1M amostras, tempo estimado: {training_time * (1000000/len(X_train_scaled))**2 / 3600:.1f} horas\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
