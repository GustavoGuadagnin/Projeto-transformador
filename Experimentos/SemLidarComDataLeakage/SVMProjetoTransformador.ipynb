{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_imports"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer\n",
        "import time\n",
        "import psutil\n",
        "from memory_profiler import memory_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_load_kaggle"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"usdot/flight-delays\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_load_data"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(path + \"/flights.csv\")\n",
        "print(f\"Dataset carregado com shape: {df.shape}\")\n",
        "print(f\"Colunas: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_data_cleaning"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Valores nulos por coluna (%):\")\n",
        "print(df.isna().sum() * 100 / len(df))\n",
        "\n",
        "delay_reason_cols = ['AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY']\n",
        "df[delay_reason_cols] = df[delay_reason_cols].fillna(0)\n",
        "\n",
        "\n",
        "df.drop(['CANCELLATION_REASON', 'FLIGHT_NUMBER', 'CANCELLED'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "df.drop('TAIL_NUMBER', axis=1, inplace=True)\n",
        "\n",
        "df.drop(['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "df = df.dropna(subset=['DEPARTURE_TIME','DEPARTURE_DELAY','TAXI_OUT','WHEELS_OFF','SCHEDULED_TIME','ELAPSED_TIME','AIR_TIME','WHEELS_ON','TAXI_IN','ARRIVAL_TIME','ARRIVAL_DELAY'])\n",
        "\n",
        "print(f\"\\nShape após limpeza: {df.shape}\")\n",
        "print(\"\\nValores nulos restantes (%):\")\n",
        "print(df.isna().sum() * 100 / len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_create_target"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['DELAYED'] = df['ARRIVAL_DELAY'].apply(lambda x: 1 if x > 15 else 0)\n",
        "\n",
        "delay = df[df['DELAYED'] == 1]\n",
        "on_time = df[df['DELAYED'] == 0]\n",
        "\n",
        "delay_count = delay.shape[0]\n",
        "on_time_count = on_time.shape[0]\n",
        "total = delay_count + on_time_count\n",
        "delay_percentage = (delay_count / total) * 100\n",
        "on_time_percentage = (on_time_count / total) * 100\n",
        "\n",
        "print(f\"Não atrasados: {on_time_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Atrasados: {delay_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Percentual de voos não atrasados: {on_time_percentage:.2f}%\")\n",
        "print(f\"Percentual de voos atrasados: {delay_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_undersampling"
      },
      "outputs": [],
      "source": [
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X = df.drop(columns=['DELAYED'])\n",
        "y = df['DELAYED']\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "df_undersampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "\n",
        "on_time_count = (df_undersampled['DELAYED'] == 0).sum()\n",
        "delay_count = (df_undersampled['DELAYED'] == 1).sum()\n",
        "total = on_time_count + delay_count\n",
        "on_time_percentage = (on_time_count / total) * 100\n",
        "delay_percentage = (delay_count / total) * 100\n",
        "\n",
        "print(f\"\\nApós undersampling:\")\n",
        "print(f\"Não atrasados: {on_time_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Atrasados: {delay_count:,}\".replace(\",\", \".\"))\n",
        "print(f\"Percentual de voos não atrasados: {on_time_percentage:.2f}%\")\n",
        "print(f\"Percentual de voos atrasados: {delay_percentage:.2f}%\")\n",
        "print(f\"Shape do dataset balanceado: {df_undersampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_sample_data"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_delayed = df_undersampled[df_undersampled['DELAYED'] == 1]\n",
        "df_on_time = df_undersampled[df_undersampled['DELAYED'] == 0]\n",
        "\n",
        "sample_size_per_class = 25000 \n",
        "\n",
        "df_delayed_sample = df_delayed.sample(n=sample_size_per_class, random_state=42)\n",
        "df_on_time_sample = df_on_time.sample(n=sample_size_per_class, random_state=42)\n",
        "\n",
        "\n",
        "df_sample = pd.concat([df_delayed_sample, df_on_time_sample])\n",
        "\n",
        "\n",
        "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Shape do DataFrame original:\", df_undersampled.shape)\n",
        "print(\"Shape do DataFrame amostrado:\", df_sample.shape)\n",
        "print(\"\\nDistribuição da classe 'DELAYED' no DataFrame amostrado:\")\n",
        "print(df_sample['DELAYED'].value_counts())\n",
        "print(f\"\\n⚠️ Limitação para {sample_size_per_class*2:,} amostras devido à complexidade computacional O(n²-n³) do SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_split_data"
      },
      "outputs": [],
      "source": [
        "X_sample = df_sample.drop(columns=['DELAYED'])\n",
        "y_sample = df_sample['DELAYED']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample, y_sample,\n",
        "    test_size=0.3,\n",
        "    stratify=y_sample,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Tamanho do conjunto de treino: {X_train.shape[0]}\")\n",
        "print(f\"Tamanho do conjunto de teste: {X_test.shape[0]}\")\n",
        "print(f\"Distribuição no treino: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Distribuição no teste: {y_test.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_normalize"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"Normalização concluída com StandardScaler\")\n",
        "print(f\"Shape treino normalizado: {X_train_scaled.shape}\")\n",
        "print(f\"Shape teste normalizado: {X_test_scaled.shape}\")\n",
        "print(f\"\\nEstatísticas após normalização (treino):\")\n",
        "print(f\"Média: {X_train_scaled.mean().mean():.6f}\")\n",
        "print(f\"Desvio padrão: {X_train_scaled.std().mean():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_train"
      },
      "outputs": [],
      "source": [
        "\n",
        "svm = SVC(\n",
        "    kernel='rbf',      \n",
        "    C=1.0,           \n",
        "    gamma='scale',     \n",
        "    random_state=42,\n",
        "    probability=True    \n",
        ")\n",
        "\n",
        "start_time_train = time.time()\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "end_time_train = time.time()\n",
        "\n",
        "training_time = end_time_train - start_time_train\n",
        "print(f\"Tempo de Treinamento: {training_time:.4f} segundos\")\n",
        "print(f\"Modelo SVM treinado com kernel '{svm.kernel}'\")\n",
        "print(f\"Parâmetro C: {svm.C}\")\n",
        "print(f\"Parâmetro gamma: {svm.gamma}\")\n",
        "print(f\"Número de vetores de suporte: {svm.n_support_}\")\n",
        "print(f\"Total de vetores de suporte: {svm.support_vectors_.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_predict"
      },
      "outputs": [],
      "source": [
        "\n",
        "start_time_pred = time.time()\n",
        "y_pred = svm.predict(X_test_scaled)\n",
        "end_time_pred = time.time()\n",
        "\n",
        "prediction_time = end_time_pred - start_time_pred\n",
        "print(f\"Tempo de Predição: {prediction_time:.4f} segundos\")\n",
        "print(f\"Predições realizadas para {len(X_test_scaled)} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_metrics"
      },
      "outputs": [],
      "source": [
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "tpr = recall  \n",
        "tnr = tn / (tn + fp)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"=== MÉTRICAS NO CONJUNTO DE TESTE ===\")\n",
        "print(f\"Acurácia: {accuracy:.4f}\")\n",
        "print(f\"Precisão: {precision:.4f}\")\n",
        "print(f\"Recall (TPR): {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"True Negative Rate (TNR): {tnr:.4f}\")\n",
        "print(\"\\nMatriz de Confusão:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_train_metrics"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train_pred = svm.predict(X_train_scaled)\n",
        "\n",
        "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
        "precision_train = precision_score(y_train, y_train_pred)\n",
        "recall_train = recall_score(y_train, y_train_pred)\n",
        "f1_train = f1_score(y_train, y_train_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
        "tpr_train = recall_train\n",
        "tnr_train = tn / (tn + fp)\n",
        "\n",
        "print(\"=== MÉTRICAS NO CONJUNTO DE TREINO ===\")\n",
        "print(f\"Acurácia: {accuracy_train:.4f}\")\n",
        "print(f\"Precisão: {precision_train:.4f}\")\n",
        "print(f\"Recall (TPR): {recall_train:.4f}\")\n",
        "print(f\"F1-score: {f1_train:.4f}\")\n",
        "print(f\"True Negative Rate (TNR): {tnr_train:.4f}\")\n",
        "\n",
        "print(\"\\n=== COMPARAÇÃO TREINO vs TESTE ===\")\n",
        "print(f\"Diferença de Acurácia: {accuracy_train - accuracy:.4f}\")\n",
        "print(f\"Diferença de F1-score: {f1_train - f1:.4f}\")\n",
        "\n",
        "# Detectando overfitting\n",
        "acc_diff = accuracy_train - accuracy\n",
        "if acc_diff > 0.05:\n",
        "    print(f\"\\n⚠️  POSSÍVEL OVERFITTING DETECTADO!\")\n",
        "    print(f\"Diferença de acurácia treino-teste: {acc_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n✅ Modelo parece estar generalizando bem.\")\n",
        "    print(f\"Diferença de acurácia treino-teste: {acc_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_cv"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALIDAÇÃO CRUZADA 5-FOLDS - SVM\")\n",
        "print(\"=\"*50)\n",
        "print(\"⚠️ Processo pode ser demorado devido à complexidade do SVM...\")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score)\n",
        "}\n",
        "\n",
        "svm_cv = SVC(\n",
        "    kernel='rbf', \n",
        "    C=1.0, \n",
        "    gamma='scale', \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "X_sample_scaled = scaler.fit_transform(X_sample)\n",
        "X_sample_scaled = pd.DataFrame(X_sample_scaled, columns=X_sample.columns)\n",
        "\n",
        "cv_results = cross_validate(svm_cv, X_sample_scaled, y_sample, cv=cv, scoring=scoring, return_train_score=True)\n",
        "\n",
        "print(f\"Acurácia - Treino: {cv_results['train_accuracy'].mean():.4f} ± {cv_results['train_accuracy'].std():.4f}\")\n",
        "print(f\"Acurácia - Validação: {cv_results['test_accuracy'].mean():.4f} ± {cv_results['test_accuracy'].std():.4f}\")\n",
        "print(f\"Precisão - Treino: {cv_results['train_precision'].mean():.4f} ± {cv_results['train_precision'].std():.4f}\")\n",
        "print(f\"Precisão - Validação: {cv_results['test_precision'].mean():.4f} ± {cv_results['test_precision'].std():.4f}\")\n",
        "print(f\"Recall - Treino: {cv_results['train_recall'].mean():.4f} ± {cv_results['train_recall'].std():.4f}\")\n",
        "print(f\"Recall - Validação: {cv_results['test_recall'].mean():.4f} ± {cv_results['test_recall'].std():.4f}\")\n",
        "print(f\"F1-score - Treino: {cv_results['train_f1'].mean():.4f} ± {cv_results['train_f1'].std():.4f}\")\n",
        "print(f\"F1-score - Validação: {cv_results['test_f1'].mean():.4f} ± {cv_results['test_f1'].std():.4f}\")\n",
        "cv_acc_diff = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
        "if cv_acc_diff > 0.05:\n",
        "    print(f\"\\n⚠️  POSSÍVEL OVERFITTING DETECTADO NA VALIDAÇÃO CRUZADA!\")\n",
        "    print(f\"Diferença de acurácia treino-validação: {cv_acc_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n✅ Modelo generalizando bem na validação cruzada.\")\n",
        "    print(f\"Diferença de acurácia treino-validação: {cv_acc_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm_performance"
      },
      "outputs": [],
      "source": [
        "process = psutil.Process()\n",
        "\n",
        "svm_perf = SVC(\n",
        "    kernel='rbf', \n",
        "    C=1.0, \n",
        "    gamma='scale', \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "def train_svm_model():\n",
        "    svm_perf.fit(X_train_scaled, y_train)\n",
        "\n",
        "start_time_train = time.time()\n",
        "mem_usage_train = memory_usage(train_svm_model)\n",
        "end_time_train = time.time()\n",
        "training_time = end_time_train - start_time_train\n",
        "train_ips = len(X_train_scaled) / training_time\n",
        "\n",
        "\n",
        "def predict_svm_model():\n",
        "    global y_pred_perf\n",
        "    y_pred_perf = svm_perf.predict(X_test_scaled)\n",
        "\n",
        "cpu_percent_before = process.cpu_percent(interval=None)\n",
        "start_time_pred = time.time()\n",
        "mem_usage_pred = memory_usage(predict_svm_model)\n",
        "end_time_pred = time.time()\n",
        "cpu_percent_after = process.cpu_percent(interval=None)\n",
        "\n",
        "prediction_time = end_time_pred - start_time_pred\n",
        "pred_ips = len(X_test_scaled) / prediction_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ANÁLISE DE DESEMPENHO COMPUTACIONAL - SVM\")\n",
        "print(\"=\"*50)\n",
        "print(f\"🕒 Tempo de Treinamento: {training_time:.4f} s\")\n",
        "print(f\"🕒 Tempo de Predição: {prediction_time:.4f} s\")\n",
        "print(f\"📈 Memória (Treinamento): {max(mem_usage_train):.2f} MB\")\n",
        "print(f\"📈 Memória (Predição): {max(mem_usage_pred):.2f} MB\")\n",
        "print(f\"⚙️ CPU usada na predição: {cpu_percent_after:.2f}%\")\n",
        "print(f\"📊 Instâncias por segundo (treinamento): {train_ips:.2f}\")\n",
        "print(f\"📊 Instâncias por segundo (predição): {pred_ips:.2f}\")\n",
        "print(f\"🔢 Vetores de suporte: {svm_perf.support_vectors_.shape[0]} ({svm_perf.support_vectors_.shape[0]/len(X_train_scaled)*100:.1f}% dos dados)\")\n",
        "\n",
        "daily_predictions = 24 * 60 * 60 * pred_ips  # predições por dia\n",
        "print(f\"\\n📈 THROUGHPUT PARA CENÁRIOS REAIS:\")\n",
        "print(f\"Predições por segundo: {pred_ips:.0f}\")\n",
        "print(f\"Predições por minuto: {pred_ips * 60:.0f}\")\n",
        "print(f\"Predições por hora: {pred_ips * 3600:.0f}\")\n",
        "print(f\"Predições por dia: {daily_predictions:.0f}\")\n",
        "\n",
        "print(f\"\\n⚠️ LIMITAÇÕES COMPUTACIONAIS:\")\n",
        "print(f\"Dataset limitado a {len(X_sample):,} amostras devido à complexidade O(n²-n³)\")\n",
        "print(f\"Para 1M amostras, tempo estimado: {training_time * (1000000/len(X_train_scaled))**2 / 3600:.1f} horas\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
